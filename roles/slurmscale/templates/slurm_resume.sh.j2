#!/bin/bash

. {{ slurm_scale_root }}/bin/activate

OS_CLOUD='{{ cloud_id }}'
OS_IDENTITY_API_VERSION=3

log_loc=/var/log/slurm/elastic.log

export OS_CLOUD OS_IDENTITY_API_VERSION

in_list () {
    key=$1
    list=$2
    for elem in ${list[@]}
    do
        [[ $key == $elem ]] && return 0
    done
    return 1
}

cd {{ slurm_scale_root }}/etc/ansible

echo "Node resume invoked: $0 $*" >> $log_loc

#echo -e "\nwrite-files:" > file_init.ci
#for file in /etc/slurm/slurm.conf /etc/munge/munge.key
#do
#  echo "$file"
#  echo -e "  - encoding: b64\n    content: $(base64 -w 0 $file)\n    owner: root:root\n    path: $file \n    permissions: '$(stat -c "%a" $file)'" >> file_init.ci
#done
#
#cat compute_init.ci file_init.ci > all_init.ci

#eh. useradd won't do anything if the user exists. just have to make sure ansible doesn't flip
# out when it 'fails' on suspend.
#echo "#!/bin/bash" > /tmp/add_users.sh
#cat /etc/passwd | awk -F':' '$4 >= 1001 && $4 < 65000 {print "useradd -M -u", $3, $1}' >> /tmp/add_users.sh


#First, loop over hosts and run the openstack create/resume commands for *all* resume hosts at once.
#Avoids getting stuck if one host fails?
spawn_list=""
start_list=""
instance_list=($(openstack server list | awk '$4 != "Name" && $4 != "" {print $4}'))
for host in $(scontrol show hostname $1)
do
    if in_list $host $instance_list; then
        start_list+="$host,"
    else
        spawn_list+="$host,"
    fi
done

# TODO: error handling here
if [[ -n $start_list ]]; then
    echo "Running ansible start on ${start_list::-1}" >> $log_loc
    ansible-playbook -v -l "${start_list::-1}" start.yml >> $log_loc
fi

if [[ -n $spawn_list ]]; then
    echo "Running ansible spawn/config on ${spawn_list::-1}" >> $log_loc
    ansible-playbook -v -l "${spawn_list::-1}" spawn.yml >> $log_loc
    ansible-playbook -v -l "${spawn_list::-1}" config.yml >> $log_loc
fi
