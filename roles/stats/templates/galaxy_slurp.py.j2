#!{{ statslurp_venv_dir }}/bin/python

import argparse
import datetime

import psycopg2
from influxdb import InfluxDBClient


TOP_USAGE_LIMIT = 30
PGCONNS = {
    'test': 'host=galaxy07.tacc.utexas.edu user=grafana password={{ galaxy_test_grafana_db_password }} dbname=galaxy_test',
    'main': 'host=galaxy-db-02.tacc.utexas.edu user=grafana password={{ galaxy_main_grafana_db_password }} dbname=galaxy_main',
}
HANDLERS = {
    'test': (
        'test_handler0',
        'test_handler1',
        'test_handler2',
        'test_handler3',
        'test_handler4',
    ),
    'main': (
        'main_w3_handler0',
        'main_w3_handler1',
        'main_w3_handler2',
        'main_w4_handler0',
        'main_w4_handler1',
        'main_w4_handler2',
    ),
}
time = datetime.datetime.utcnow().strftime('%Y-%m-%dT%H:%M:%SZ')


def sql_common_filters():
    return """
                NOT purged
            AND
                disk_usage IS NOT NULL
            AND
                id NOT IN (SELECT user_id FROM user_group_association)
            AND
                id NOT IN (SELECT user_id FROM user_quota_association)
    """

def sql_min_usage_filter(min_usage=0):
    if not min_usage:
        return ''
    return """
            AND disk_usage > {min_usage}
    """.format(
        min_usage=min_usage - 1
    )


def sql_days_filter(days=0):
    if not days:
        return ''
    return """
            AND update_time > NOW() - interval '{days} days'
    """.format(
        days=days,
    )


def sql_top_usage(days=0):
    return """
        SELECT
            id,
            email,
            disk_usage
        FROM
            galaxy_user
        WHERE
            {common_filters}
            {days_filter}
        ORDER BY
            disk_usage DESC
        LIMIT
            {top_usage_limit}
    """.format(
        common_filters=sql_common_filters(),
        days_filter=sql_days_filter(days=days),
        top_usage_limit=TOP_USAGE_LIMIT,
    )


def sql_avg_usage(min_usage=0, days=0):
    return """
        SELECT
            avg(disk_usage)
        FROM
            galaxy_user
        WHERE
            {common_filters}
            {min_usage_filter}
            {days_filter}
    """.format(
        common_filters=sql_common_filters(),
        min_usage_filter=sql_min_usage_filter(min_usage=min_usage),
        days_filter=sql_days_filter(days=days),
    )


def sql_sum_usage():
    return """
        SELECT
            sum(total_size)
        FROM
            dataset
        WHERE
            NOT purged
    """


def sql_count_internally_queued_jobs():
    return """
        SELECT
            handler,
            count(handler)
        FROM
            job
        WHERE
            state = 'queued'
            AND job_runner_external_id IS null
        GROUP BY
            handler
    """


def sql_count_total_queued_jobs():
    return """
        SELECT
            sum(CASE WHEN job_runner_external_id IS NOT null THEN 1 ELSE 0 END),
            sum(CASE WHEN job_runner_external_id IS null THEN 1 ELSE 0 END)
        FROM
            job
        WHERE
            state = 'queued'
    """


def make_measurement(measurement, value, tags=None):
    m = {
        'measurement': measurement,
        'time': time,
        'fields': {
            'value': value
        }
    }
    if tags:
        m['tags'] = tags
    return m


def pg_execute(pconn_str, sql):
    pconn = psycopg2.connect(pconn_str)
    pc = pconn.cursor()
    pc.execute(sql)
    for row in pc:
        yield row
    pconn.close()


def get_internally_queued_jobs(instance):
    measurements = []
    counts = {}
    pconn_str = PGCONNS[instance]
    for handler in HANDLERS[instance]:
        counts[handler] = 0
    for handler, count in pg_execute(pconn_str, sql_count_internally_queued_jobs()):
        counts[handler] = int(count)
    for handler, count in counts.items():
        measurements.append(make_measurement('jobs_queued_internal_by_handler', count, tags={'handler': handler}))
    return measurements


def collect(instance):
    measurements = []
    pconn_str = PGCONNS[instance]
    for days in (0, 30, 90, 180, 365):
        for min_usage in (0, 1, 1024*1024*1024):
            row = next(pg_execute(pconn_str, sql_avg_usage(min_usage=min_usage, days=days)))
            tags = {
                'min': min_usage,
                'days': days,
            }
            if row[0]:
                measurements.append(make_measurement('disk_usage_mean', float(row[0]), tags=tags))
    for days in (0, 30, 90, 180, 365):
        for row in pg_execute(pconn_str, sql_top_usage(days=days)):
            tags = {
                'id': row[0],
                'email': row[1],
                'days': days,
            }
            if row[0]:
                measurements.append(make_measurement('disk_usage_top', float(row[2]), tags=tags))
    measurements.append(
        make_measurement(
            'disk_usage_sum',
            int(next(pg_execute(pconn_str, sql_sum_usage()))[0]),
        )
    )
    queued_external, queued_internal = [int(i) for i in next(pg_execute(pconn_str, sql_count_total_queued_jobs()))]
    measurements.append(make_measurement('jobs_queued_external', queued_external))
    measurements.append(make_measurement('jobs_queued_internal', queued_internal))
    measurements.extend(get_internally_queued_jobs(instance))
    return measurements


def dump(instance, points):
    db = '{instance}_sql'.format(instance=instance)
    client = InfluxDBClient(database=db)
    client.create_database(db)
    client.write_points(points)


def main():
    parser = argparse.ArgumentParser(description="Translate Galaxy DB stats from PostgreSQL to InfluxDB")
    parser.add_argument('instance', default='main', help="Galaxy instance")
    args = parser.parse_args()
    points = collect(args.instance)
    dump(args.instance, points)


if __name__ == '__main__':
    main()
