##
## This file is maintained by Ansible - CHANGES WILL BE OVERWRITTEN
##
ControlMachine={{ slurm_controller_name }}
ControlAddr={{ slurm_controller_ip }}
#
AuthType=auth/munge
FastSchedule=1
JobCompLoc=/var/log/slurm/slurm.job.log
JobCompType=jobcomp/filetxt
PluginDir=/usr/lib64/slurm
SchedulerType=sched/backfill
#SelectType=select/cons_res
#SelectTypeParameters=CR_CPU_Memory
SelectType=select/linear
SlurmUser=slurm
SlurmctldPort=7002
SlurmctldTimeout=300
SlurmdPort=7003
SlurmdSpoolDir=/var/lib/slurm/slurmd/slurmd.spool
SlurmdTimeout=300
StateSaveLocation=/var/lib/slurm/slurmctld/slurm.state
SwitchType=switch/none
DefaultStorageLoc=/var/log/slurm/slurm_accounting
AccountingStorageType=accounting_storage/slurmdbd
AccountingStorageHost=galaxy08.tacc.utexas.edu
#AccountingStoragePort=6819
AccountingStoragePort=7031
JobAcctGatherType=jobacct_gather/linux
JobAcctGatherFrequency=task=15
ProctrackType=proctrack/linuxproc
ClusterName={{ slurm_cluster_name }}
ReturnToService=1
#
SlurmctldLogFile=/var/log/slurm/slurmctld.log
SlurmctldDebug=7
SlurmdLogFile=/var/log/slurm/slurmd.log
SlurmdDebug=6
#
# TODO: redeployed controller, any way to do this dynamically?
FirstJobId=592574
#
# Node Configurations
#
NodeName={{ slurm_cluster_name }}-small[0-8] State=CLOUD CPUs=2
NodeName={{ slurm_cluster_name }}-medium[0-8] State=CLOUD CPUs=6
NodeName={{ slurm_cluster_name }}-large[0-8] State=CLOUD CPUs=10
#
# Partition Configurations
#
PartitionName=small Nodes={{ slurm_cluster_name }}-small[0-8] Default=YES DefaultTime=48:20:00 MaxTime=96:20:00 MaxNodes=1 State=UP
PartitionName=normal Nodes={{ slurm_cluster_name }}-medium[0-8] DefaultTime=48:20:00 MaxTime=96:20:00 MaxNodes=1 State=UP
PartitionName=multi Nodes={{ slurm_cluster_name }}-large[0-8] DefaultTime=48:20:00 MaxTime=96:20:00 MaxNodes=1 State=UP
